# -*- coding: utf-8 -*-
"""IA4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wfRVPHUSSjcjPvibMSatB1fBCvp-nFBX
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import math
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score

# add more imports if necessary

from google.colab import drive
drive.mount('/content/gdrive')
EMBEDDING_PATH = '/content/gdrive/My Drive/AI534/GloVe_Embedder_data.txt'

class GloVe_Embedder:
    def __init__(self, path):
        self.embedding_dict = {}
        self.embedding_array = []
        self.unk_emb = 0
        # Adapted from https://stackoverflow.com/questions/37793118/load-pretrained-GloVe-vectors-in-python
        with open(path,'r') as f:
            for line in f:
                split_line = line.split()
                word = split_line[0]
                embedding = np.array(split_line[1:], dtype=np.float64)
                self.embedding_dict[word] = embedding
                self.embedding_array.append(embedding.tolist())
        self.embedding_array = np.array(self.embedding_array)
        self.embedding_dim = len(self.embedding_array[0])
        self.vocab_size = len(self.embedding_array)
        self.unk_emb = np.zeros(self.embedding_dim)

    # Check if the provided embedding is the unknown embedding.
    def is_unk_embed(self, embed):
        return np.sum((embed - self.unk_emb) ** 2) < 1e-7

    # Check if the provided string is in the vocabulary.
    def token_in_vocab(self, x):
        if x in self.embedding_dict and not self.is_unk_embed(self.embedding_dict[x]):
            return True
        return False

    # Returns the embedding for a single string and prints a warning if
    # the string is unknown to the vocabulary.
    #
    # If indicate_unk is set to True, the return type will be a tuple of
    # (numpy array, bool) with the bool indicating whether the returned
    # embedding is the unknown embedding.
    #
    # If warn_unk is set to False, the method will no longer print warnings
    # when used on unknown strings.
    def embed_str(self, x, indicate_unk = False, warn_unk = True):
        if self.token_in_vocab(x):
            if indicate_unk:
                return (self.embedding_dict[x], False)
            else:
                return self.embedding_dict[x]
        else:
            if warn_unk:
                    print("Warning: provided word is not part of the vocabulary!")
            if indicate_unk:
                return (self.unk_emb, True)
            else:
                return self.unk_emb

    # Returns an array containing the embeddings of each vocabulary token in the provided list.
    #
    # If include_unk is set to False, the returned list will not include any unknown embeddings.
    def embed_list(self, x, include_unk = True):
        if include_unk:
            embeds = [self.embed_str(word, warn_unk = False).tolist() for word in x]
        else:
            embeds_with_unk = [self.embed_str(word, indicate_unk=True, warn_unk = False) for word in x]
            embeds = [e[0].tolist() for e in embeds_with_unk if not e[1]]
            if len(embeds) == 0:
                print("No known words in input:" + str(x))
                embeds = [self.unk_emb.tolist()]
        return np.array(embeds)

    # Finds the vocab words associated with the k nearest embeddings of the provided word.
    # Can also accept an embedding vector in place of a string word.
    # Return type is a nested list where each entry is a word in the vocab followed by its
    # distance from whatever word was provided as an argument.
    def find_k_nearest(self, word, k, warn_about_unks = True):
        if type(word) == str:
            word_embedding, is_unk = self.embed_str(word, indicate_unk = True)
        else:
            word_embedding = word
            is_unk = False
        if is_unk and warn_about_unks:
            print("Warning: provided word is not part of the vocabulary!")

        all_distances = np.sum((self.embedding_array - word_embedding) ** 2, axis = 1) ** 0.5
        distance_vocab_index = [[w, round(d, 5)] for w,d,i in zip(self.embedding_dict.keys(), all_distances, range(len(all_distances)))]
        distance_vocab_index = sorted(distance_vocab_index, key = lambda x: x[1], reverse = False)
        return distance_vocab_index[:k]

    def save_to_file(self, path):
        with open(path, 'w') as f:
            for k in self.embedding_dict.keys():
                embedding_str = " ".join([str(round(s, 5)) for s in self.embedding_dict[k].tolist()])
                string = k + " " + embedding_str
                f.write(string + "\n")

# Your code goes here
ge = GloVe_Embedder(EMBEDDING_PATH)
#
# Embed single word via:
embed = ge.embed_str('flight')
#
# Embed a list of words via:
wordss = ['flight', 'awesome', 'terrible', 'help', 'late']
embeds = ge.embed_list(wordss)
print(ge.find_k_nearest('flight', 30))

# Find k nearest neighbors of word via:
all_150_words = pd.DataFrame()
for i in wordss:
  res = pd.DataFrame(ge.find_k_nearest(i, 30), columns=['word', 'weight'])
  res['seed_word'] = i

  all_150_words = pd.concat([all_150_words, res], ignore_index=True)
print(all_150_words)

# Save vocabulary to file via:
ge.save_to_file('/content/gdrive/My Drive/AI534/all_words.csv')

embeds_for_150 = ge.embed_list(all_150_words['word'])

seed_words = all_150_words['seed_word']

scaler = StandardScaler()
embeddings_standardized = scaler.fit_transform(embeds_for_150)
pca = PCA(n_components=2)
embeddings_2d = pca.fit_transform(embeddings_standardized)
all_150_words_2d = pd.DataFrame(data=embeddings_2d, columns=['PC1', 'PC2'])

all_150_words_2d['seed_word'] = seed_words

# Display the resulting DataFrame
print(all_150_words_2d)

combined_df = all_150_words_2d

# Plotting
plt.figure(figsize=(15, 10))
colors = {'flight': 'red', 'awesome': 'blue', 'terrible': 'green', 'help': 'orange', 'late': 'purple'}
for seed_word in combined_df['seed_word'].unique():
    subset = combined_df[combined_df['seed_word'] == seed_word]
    plt.scatter(subset['PC1'], subset['PC2'], label=seed_word, color=colors[seed_word])

plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.title('2D PCA of Word Embeddings')
plt.legend()
plt.show()

perplexities = [5, 10, 20, 30, 40, 50]
plt.figure(figsize=(15, 10))

for i, perplexity in enumerate(perplexities):
    tsne = TSNE(n_components=2, perplexity=perplexity, random_state=42)
    embeddings_2d = tsne.fit_transform(embeddings_standardized)
    all_150_words_2d = pd.DataFrame(data=embeddings_2d, columns=['Dimension 1', 'Dimension 2'])
    all_150_words_2d['seed_word'] = seed_words
    combined_df = all_150_words_2d

    plt.subplot(2, 3, i + 1)
    colors = {'flight': 'red', 'awesome': 'blue', 'terrible': 'green', 'help': 'orange', 'late': 'purple'}
    for seed_word in combined_df['seed_word'].unique():
        subset = combined_df[combined_df['seed_word'] == seed_word]
        plt.scatter(subset['Dimension 1'], subset['Dimension 2'], label=seed_word, color=colors[seed_word])
    plt.title(f't-SNE with Perplexity={perplexity}')
    plt.xlabel('Dimension 1')
    plt.ylabel('Dimension 2')

plt.tight_layout()
plt.show()

kmeans_objectives = []

for k in range(2, 21):
    kmeans = KMeans(n_clusters=k,n_init=10)
    kmeans.fit(embeds_for_150)
    kmeans_objectives.append(kmeans.inertia_)

kmeans_results = pd.DataFrame({'k': range(2, 21), 'inertia': kmeans_objectives})
kmeans_results

plt.plot(kmeans_results['k'], kmeans_results['inertia'], marker='o')
plt.title('Dmitrys and Joshua K-Means kalapolooza')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Inertia')
plt.show()

true_labels = all_150_words['seed_word']\
      #a function to calculate purity
def calculate_purity(true_labels, predicted_labels):
    contingency_matrix = np.zeros((len(np.unique(true_labels)), len(np.unique(predicted_labels))))

    for i, true_label in enumerate(np.unique(true_labels)):
        true_indices = (true_labels == true_label)
        predicted_cluster = np.argmax(np.bincount(predicted_labels[true_indices]))
        contingency_matrix[i, predicted_cluster] = np.sum(true_indices)

    return np.sum(np.amax(contingency_matrix, axis=0)) / len(true_labels)

for k in range(2, 21):
    # Apply k-means clustering
    kmeans = KMeans(n_clusters=k, n_init=100)
    predicted_labels = kmeans.fit_predict(embeds_for_150)  # Use the embeddings_array from the previous example

    # Evaluate clustering solution using different metrics
    purity = calculate_purity(true_labels, predicted_labels)
    ari = adjusted_rand_score(true_labels, predicted_labels)
    nmi = normalized_mutual_info_score(true_labels, predicted_labels)
    print(f'For k={k}: Purity = {purity:.4f}, ARI = {ari:.4f}, NMI = {nmi:.4f}')

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from nltk.tokenize import word_tokenize
import nltk
#nltk.download('punkt')

train_path = '/content/gdrive/My Drive/AI534/IA3-train.csv' # DO NOT MODIFY THIS. Please make sure your data has this exact path
val_path = '/content/gdrive/My Drive/AI534/IA3-dev.csv' # DO NOT MODIFY THIS. Please make sure your data has this exact path
train_data = pd.read_csv(train_path)
val_data = pd.read_csv(val_path)
X_train_text = train_data['text']
y_train = train_data['sentiment']
X_val_text = val_data['text']
y_val = val_data['sentiment']

def tokenize_tweet(tweet):
    return word_tokenize(tweet)

X_train_words = X_train_text.apply(tokenize_tweet)
X_val_words = X_val_text.apply(tokenize_tweet)

embeds_train = []

for words in X_train_words:
    embeddings = ge.embed_list(words)
    avg_embedding = np.mean(embeddings, axis=0)
    embeds_train.append(avg_embedding)

embeds_train = np.array(embeds_train)

embeds_val = []

for words in X_val_words:
    embeddings = ge.embed_list(words)
    avg_embedding = np.mean(embeddings, axis=0)
    embeds_val.append(avg_embedding)

embeds_val = np.array(embeds_val)


# Initialize Random Forest model
rf_model = RandomForestClassifier(n_estimators=100, min_samples_split=5)
rf_model.fit(embeds_train, y_train)

# Predictions on the validation set
y_pred_val = rf_model.predict(embeds_val)

# Evaluate the model
print(classification_report(y_val, y_pred_val))